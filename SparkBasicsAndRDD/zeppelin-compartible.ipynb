{
  "metadata": {
    "name": "Glue PySpark",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nimport collections\nfrom dataclasses import dataclass, field\n\n# Create a SparkSession\nspark \u003d SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\nsc \u003d spark.sparkContext"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom pyspark import SparkConf, SparkContext \nimport collections\nfrom dataclasses import dataclass, field\nfrom awsglue.context import GlueContext\n\nconf \u003d SparkConf().setMaster(\"local\").setAppName(\"udemy\")\nsc \u003d SparkContext.getOrCreate(conf\u003dconf)\nglueContext \u003d GlueContext(sc)\nspark \u003d glueContext.spark_session"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# RDD: mapValue and reduceByKey function"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nlines \u003d sc.textFile(\"s3://thanhtt-0000-datalake/udemy/ml-100k/u.data\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nratings \u003d lines.map(lambda x: x.split()[2])"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nresult \u003d ratings.countByValue()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nsortedResults \u003d collections.OrderedDict(sorted(result.items()))\nfor key, value in sortedResults.items():\n    print(\"%s %i\" % (key, value))"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndef parseLine(line):\n    fields \u003d line.split(\u0027,\u0027)\n    age \u003d int(fields[2])\n    numFriends \u003d int(fields[3])\n    return (age, numFriends)\n\nlines \u003d sc.textFile(\"s3://370784835428-datalake/udemy/SparkCourse/SparkCourse/fakefriends.csv\")\nrdd \u003d lines.map(parseLine) \n\ntotalsByAge \u003d rdd.mapValues(lambda x:(x, 1)).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \n\ntotalsByAgeCollected \u003d totalsByAge.collect() \n\naveragesByAge \u003d totalsByAge.mapValues(lambda x:x[0]/x[1]) \n\nresults \u003d averagesByAge.collect()\n  \nfor result in results:\n   print(result)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# RDD: filter function"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "def parseLine(line):\n    fields \u003d line.split(\u0027,\u0027)\n    stationID \u003d fields[0]\n    entryType \u003d fields[2]\n    temperature \u003d float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n    return (stationID, entryType, temperature)\n\nlines \u003d sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/1800.csv\")\nparsedLines \u003d lines.map(parseLine)\nminTemps \u003d parsedLines.filter(lambda x: \"TMIN\" in x[1])\nstationTemps \u003d minTemps.map(lambda x: (x[0], x[2]))\nminTemps \u003d stationTemps.reduceByKey(lambda x, y: min(x,y))\nresults \u003d minTemps.collect();\n\nfor result in results:\n    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# RDD: flatMap function"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "import re\n\ndef normalizeWords(text):\n    return re.compile(r\u0027\\W+\u0027,re.UNICODE).split(text.lower())\n\ninput \u003d sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\nwords \u003d input.flatMap(normalizeWords) # the text split into many rows\n\nwordCounts \u003d words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\nwordCountsSorted \u003d wordCounts.map(lambda x: (x[0],x[1])).map(lambda x:(x[1],x[0]) ).sortByKey()\n# for item in wordCountsSorted.collect():# comment because the list is too long\n    # print(item)\nprint(\"Result is commented because it\u0027s too long\")\nresults \u003d wordCountsSorted.collect()\n\n# for result in results:\n#     count \u003d str(result[0]) \n#     word \u003d result[1].encode(\"ascii\",\"ignore\")\n    \n#     # comment because the list is too long\n#     if(word):\n#         print(cleanWord,count)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# **DataFrame: Spark SQL**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "from pyspark.sql import Row\n\ndef mapper(line):\n    fields \u003d line.split(\u0027,\u0027)\n    return Row(ID \u003d int(fields[0]), name \u003d str(fields[1].encode(\"utf-8\")),\n               age \u003d int(fields[2]), numFriends \u003d int(fields[3]))\n\nlines \u003d sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends.csv\")\npeople \u003d lines.map(mapper)\n\n# Infer the schema, and register the DataFrame as table.\nschemaPeople \u003d spark.createDataFrame(people).cache()\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been register as a table\nteenagers \u003d spark.sql(\"SELECT * FROM people WHERE age \u003e\u003d13 and age \u003c\u003d19\")\n\n# The results of SQL queries are RDDs and support all the normal RDD operations\nfor teen in teenagers.collect():\n    print(teen)\n    \n# We can also use functions instead of SQL queries\nschemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# **DataFrame: Infer Schema and common function to work with**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "from pyspark.sql import functions as func\n\npeople \u003d spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n            .csv(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends-header.csv\")\n\nprint(\"here is our inferred schema\")\npeople.printSchema()\n\nprint(\"display the name column\")\npeople.select(\"name\").show()\n\nprint(\"filter out anyone over 21\")\npeople.filter(people.age \u003c 21).show()\n\nprint(\"group by age\")\npeople.groupBy(\"age\").count().show()\n\nprint(\"make everyone 10 year older\")\npeople.select(people.name, people.age +10).show()\n\nprint(\"sorted\")\nfriendByAge \u003d people.select(\"age\",\"friends\")\nfriendByAge.groupBy(\"age\").avg(\"friends\").sort(\"age\").show()\n\nprint(\"formatted more nicely\")\nfriendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2)).sort(\"age\").show()\n\nprint(\"with a custom column name\")\nfriendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2).alias(\"friends_avg\")).sort(\"age\").show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# **Word count with DataFrame(split unstructure text into multi row dataframe)**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# read each line of my book into a dataframe\ninputDF \u003d spark.read.text(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\ninputDF.show()\n\n# split using a regular expression that extract words\nwords \u003d inputDF.select(func.explode(func.split(inputDF.value,\"\\\\W+\")).alias(\"word\"))\nwords.filter(words.word !\u003d \"\")\n\nwords.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}