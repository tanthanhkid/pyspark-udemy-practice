{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "from pyspark import SparkConf, SparkContext \nimport collections\nfrom dataclasses import dataclass, field\nfrom awsglue.context import GlueContext\n\nconf = SparkConf().setMaster(\"local\").setAppName(\"udemy\")\nsc = SparkContext.getOrCreate(conf=conf)\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::441656060858:role/AWSGlueServiceRole-thanhtt-datalake\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: d5ef52a9-abcd-4871-99df-71c5bd827c7b\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session d5ef52a9-abcd-4871-99df-71c5bd827c7b to get into ready status...\nSession d5ef52a9-abcd-4871-99df-71c5bd827c7b has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "53d41df8"
		},
		{
			"cell_type": "markdown",
			"source": "# RDD: mapValue and reduceByKey function",
			"metadata": {},
			"id": "4a6be477-a10c-443d-8a6e-397ca2dfdc26"
		},
		{
			"cell_type": "code",
			"source": "lines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/ml-100k/u.data\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "0ddc3c29"
		},
		{
			"cell_type": "code",
			"source": "ratings = lines.map(lambda x: x.split()[2])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "0913d2a4"
		},
		{
			"cell_type": "code",
			"source": "result = ratings.countByValue()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "3520697b"
		},
		{
			"cell_type": "code",
			"source": "sortedResults = collections.OrderedDict(sorted(result.items()))\nfor key, value in sortedResults.items():\n    print(\"%s %i\" % (key, value))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "1 6110\n2 11370\n3 27145\n4 34174\n5 21201\n",
					"output_type": "stream"
				}
			],
			"id": "ba9ace9f"
		},
		{
			"cell_type": "markdown",
			"source": "def parseLine(line):\n    fields = line.split(',')\n    age = int(fields[2])\n    numFriends = int(fields[3])\n    return (age, numFriends)\n\nlines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends.csv\")\nrdd = lines.map(parseLine) \n\ntotalsByAge = rdd.mapValues(lambda x:(x, 1)).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \n\ntotalsByAgeCollected = totalsByAge.collect() \n\naveragesByAge = totalsByAge.mapValues(lambda x:x[0]/x[1]) \n\nresults = averagesByAge.collect()\n  \nfor result in results:\n   print(result)",
			"metadata": {},
			"id": "7c294774-620b-44fa-aacd-3e883dff0dc7"
		},
		{
			"cell_type": "markdown",
			"source": "",
			"metadata": {},
			"id": "d2c14ec0-3d06-448e-94df-d837ceae1c83"
		},
		{
			"cell_type": "markdown",
			"source": "# RDD: filter function",
			"metadata": {},
			"id": "0e769dcf-c481-4986-aaa6-da75efcb2f4f"
		},
		{
			"cell_type": "markdown",
			"source": "def parseLine(line):\n    fields = line.split(',')\n    stationID = fields[0]\n    entryType = fields[2]\n    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n    return (stationID, entryType, temperature)\n\nlines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/1800.csv\")\nparsedLines = lines.map(parseLine)\nminTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\nstationTemps = minTemps.map(lambda x: (x[0], x[2]))\nminTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\nresults = minTemps.collect();\n\nfor result in results:\n    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
			"metadata": {},
			"id": "8bbc7da2-e839-4629-bcb6-14921ccc3359"
		},
		{
			"cell_type": "markdown",
			"source": "# RDD: flatMap function",
			"metadata": {},
			"id": "809d32c5-4c97-42e0-88ee-923d7fc40253"
		},
		{
			"cell_type": "markdown",
			"source": "import re\n\ndef normalizeWords(text):\n    return re.compile(r'\\W+',re.UNICODE).split(text.lower())\n\ninput = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\nwords = input.flatMap(normalizeWords) # the text split into many rows\n\nwordCounts = words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\nwordCountsSorted = wordCounts.map(lambda x: (x[0],x[1])).map(lambda x:(x[1],x[0]) ).sortByKey()\n# for item in wordCountsSorted.collect():# comment because the list is too long\n    # print(item)\nprint(\"Result is commented because it's too long\")\nresults = wordCountsSorted.collect()\n\n# for result in results:\n#     count = str(result[0]) \n#     word = result[1].encode(\"ascii\",\"ignore\")\n    \n#     # comment because the list is too long\n#     if(word):\n#         print(cleanWord,count)\n",
			"metadata": {},
			"id": "ab7f3664-4b90-405f-817a-29e3cc3a9816"
		},
		{
			"cell_type": "markdown",
			"source": "# **DataFrame: Spark SQL**",
			"metadata": {},
			"id": "5de8b99a-ee39-441b-aa6c-5bdc11f10fde"
		},
		{
			"cell_type": "markdown",
			"source": "from pyspark.sql import Row\n\ndef mapper(line):\n    fields = line.split(',')\n    return Row(ID = int(fields[0]), name = str(fields[1].encode(\"utf-8\")),\n               age = int(fields[2]), numFriends = int(fields[3]))\n\nlines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends.csv\")\npeople = lines.map(mapper)\n\n# Infer the schema, and register the DataFrame as table.\nschemaPeople = spark.createDataFrame(people).cache()\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been register as a table\nteenagers = spark.sql(\"SELECT * FROM people WHERE age >=13 and age <=19\")\n\n# The results of SQL queries are RDDs and support all the normal RDD operations\nfor teen in teenagers.collect():\n    print(teen)\n    \n# We can also use functions instead of SQL queries\nschemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
			"metadata": {},
			"id": "3c6754ca-aadc-4609-906b-0fb52e2c5363"
		},
		{
			"cell_type": "markdown",
			"source": "# **DataFrame: Infer Schema and common function to work with**",
			"metadata": {},
			"id": "b91d043a-5044-4ff1-8bff-43670bb03a2c"
		},
		{
			"cell_type": "markdown",
			"source": "from pyspark.sql import functions as func\n\npeople = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n            .csv(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends-header.csv\")\n\nprint(\"here is our inferred schema\")\npeople.printSchema()\n\nprint(\"display the name column\")\npeople.select(\"name\").show()\n\nprint(\"filter out anyone over 21\")\npeople.filter(people.age < 21).show()\n\nprint(\"group by age\")\npeople.groupBy(\"age\").count().show()\n\nprint(\"make everyone 10 year older\")\npeople.select(people.name, people.age +10).show()\n\nprint(\"sorted\")\nfriendByAge = people.select(\"age\",\"friends\")\nfriendByAge.groupBy(\"age\").avg(\"friends\").sort(\"age\").show()\n\nprint(\"formatted more nicely\")\nfriendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2)).sort(\"age\").show()\n\nprint(\"with a custom column name\")\nfriendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2).alias(\"friends_avg\")).sort(\"age\").show()\n",
			"metadata": {},
			"id": "7850b84d-20e1-4f59-adab-70ed8e111243"
		},
		{
			"cell_type": "markdown",
			"source": "# **Word count with DataFrame(split unstructure text into multi row dataframe)**",
			"metadata": {},
			"id": "a611ee7b-3000-4923-9246-7baaf204104e"
		},
		{
			"cell_type": "markdown",
			"source": "# read each line of my book into a dataframe\ninputDF = spark.read.text(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\ninputDF.show()\n\n# split using a regular expression that extract words\nwords = inputDF.select(func.explode(func.split(inputDF.value,\"\\\\W+\")).alias(\"word\"))\nwords.filter(words.word != \"\")\n\nwords.show()",
			"metadata": {},
			"id": "6c2c1df2-0b7e-4866-a932-0b4e31e60771"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "81d0beed-2a9f-42e3-8c88-2e302bf9fc9e"
		}
	]
}