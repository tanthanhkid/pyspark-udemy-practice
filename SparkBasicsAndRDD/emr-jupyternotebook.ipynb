{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d41df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "from dataclasses import dataclass, field \n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6be477-a10c-443d-8a6e-397ca2dfdc26",
   "metadata": {},
   "source": [
    "# RDD: mapValue and reduceByKey function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"s3://370784835428-datalake/udemy/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = lines.map(lambda x: x.split()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ratings.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ace9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c9de6-0e78-43a2-b347-645cc529d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"s3://370784835428-datalake/udemy/SparkCourse/SparkCourse/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine) \n",
    "\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1)).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \n",
    "\n",
    "totalsByAgeCollected = totalsByAge.collect() \n",
    "\n",
    "averagesByAge = totalsByAge.mapValues(lambda x:x[0]/x[1]) \n",
    "\n",
    "results = averagesByAge.collect()\n",
    "  \n",
    "for result in results:\n",
    "   print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14ec0-3d06-448e-94df-d837ceae1c83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e769dcf-c481-4986-aaa6-da75efcb2f4f",
   "metadata": {},
   "source": [
    "# RDD: filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f38c5-fee3-4630-8eda-20dff7ebb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d32c5-4c97-42e0-88ee-923d7fc40253",
   "metadata": {},
   "source": [
    "# RDD: flatMap function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e04dd6-7e9e-4b83-96e1-272e57adc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+',re.UNICODE).split(text.lower())\n",
    "\n",
    "input = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\n",
    "words = input.flatMap(normalizeWords) # the text split into many rows\n",
    "\n",
    "wordCounts = words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[0],x[1])).map(lambda x:(x[1],x[0]) ).sortByKey()\n",
    "# for item in wordCountsSorted.collect():# comment because the list is too long\n",
    "    # print(item)\n",
    "print(\"Result is commented because it's too long\")\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "# for result in results:\n",
    "#     count = str(result[0]) \n",
    "#     word = result[1].encode(\"ascii\",\"ignore\")\n",
    "    \n",
    "#     # comment because the list is too long\n",
    "#     if(word):\n",
    "#         print(cleanWord,count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b99a-ee39-441b-aa6c-5bdc11f10fde",
   "metadata": {},
   "source": [
    "# **DataFrame: Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dcaf8-82bc-461e-b4f8-9b7a9dabe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID = int(fields[0]), name = str(fields[1].encode(\"utf-8\")),\n",
    "               age = int(fields[2]), numFriends = int(fields[3]))\n",
    "\n",
    "lines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "\n",
    "# Infer the schema, and register the DataFrame as table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been register as a table\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >=13 and age <=19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)\n",
    "    \n",
    "# We can also use functions instead of SQL queries\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d043a-5044-4ff1-8bff-43670bb03a2c",
   "metadata": {},
   "source": [
    "# **DataFrame: Infer Schema and common function to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f44e3-250c-4793-8f76-0e0fddaa4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "people = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "            .csv(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends-header.csv\")\n",
    "\n",
    "print(\"here is our inferred schema\")\n",
    "people.printSchema()\n",
    "\n",
    "print(\"display the name column\")\n",
    "people.select(\"name\").show()\n",
    "\n",
    "print(\"filter out anyone over 21\")\n",
    "people.filter(people.age < 21).show()\n",
    "\n",
    "print(\"group by age\")\n",
    "people.groupBy(\"age\").count().show()\n",
    "\n",
    "print(\"make everyone 10 year older\")\n",
    "people.select(people.name, people.age +10).show()\n",
    "\n",
    "print(\"sorted\")\n",
    "friendByAge = people.select(\"age\",\"friends\")\n",
    "friendByAge.groupBy(\"age\").avg(\"friends\").sort(\"age\").show()\n",
    "\n",
    "print(\"formatted more nicely\")\n",
    "friendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2)).sort(\"age\").show()\n",
    "\n",
    "print(\"with a custom column name\")\n",
    "friendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2).alias(\"friends_avg\")).sort(\"age\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611ee7b-3000-4923-9246-7baaf204104e",
   "metadata": {},
   "source": [
    "# **Word count with DataFrame(split unstructure text into multi row dataframe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333cde4-2741-4297-8073-0895cc7c702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each line of my book into a dataframe\n",
    "inputDF = spark.read.text(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\n",
    "inputDF.show()\n",
    "\n",
    "# split using a regular expression that extract words\n",
    "words = inputDF.select(func.explode(func.split(inputDF.value,\"\\\\W+\")).alias(\"word\"))\n",
    "words.filter(words.word != \"\")\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f4395-4d49-497e-ac2b-18422e12711e",
   "metadata": {},
   "source": [
    "# **DataFrame: StructType, StructField, IntegerType, FloatType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8577ce-cb9f-439b-8cad-a29c766a89a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "#create schema when reading customer order\n",
    "customerOrderSchema = StructType([\\\n",
    "                                 StructField(\"cust_id\", IntegerType(), True),\n",
    "                                 StructField(\"item_id\", IntegerType(), True),\n",
    "                                 StructField(\"amount_spent\", FloatType(), True)\n",
    "                                 ])\n",
    "\n",
    "#load up the data into spark dataset\n",
    "customerDF = spark.read.schema(customerOrderSchema).csv(\"s3://370784835428-datalake/udemy/SparkCourse/SparkCourse/customer-orders.csv\")\n",
    "\n",
    "totalByCustomer = customerDF.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"),2).alias(\"total_spent\"))\n",
    "\n",
    "totalByCustomerSorted = totalByCustomer.sort(\"total_spent\")\n",
    "\n",
    "totalByCustomer.show(totalByCustomerSorted.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cdf11-0424-4ed1-84c4-797d7eb2b670",
   "metadata": {},
   "source": [
    "# **DataFrame: OrderBy function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc918b-f6cd-428d-8705-99fcc3fcf407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "#create schema when reading customer order\n",
    "schema = StructType([\\\n",
    "             StructField(\"userID\", IntegerType(), True),\n",
    "             StructField(\"movieID\", IntegerType(), True),\n",
    "             StructField(\"rating\", IntegerType(), True),\n",
    "             StructField(\"timestamp\", LongType(), True)\n",
    "             ])\n",
    "\n",
    "#load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\",\"\\t\").schema(schema).csv(\"s3://370784835428-datalake/udemy/u.data\")\n",
    "\n",
    "# some SQL-style magic to sort all the movies by popularity in one line\n",
    "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "# grab the top 10\n",
    "topMovieIDs.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2739ab7-b6b1-445e-aab1-474145f8bec2",
   "metadata": {},
   "source": [
    "# **DataFrame: broadcast function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbb24b-6fb3-4c0b-91d8-49c7c543adb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install boto3 pythob library for download file from S3 \n",
    "sc.install_pypi_package(\"boto3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a739e-9ed9-495e-85ea-f2a13b63b2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move current dir to /tmp and check all directory\n",
    "os.chdir(\"/tmp\")\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f5a7d7-9b1b-4d53-aef1-4e88600bf644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T12:09:15.105287Z",
     "iopub.status.busy": "2023-02-01T12:09:15.105038Z",
     "iopub.status.idle": "2023-02-01T12:09:44.586675Z",
     "shell.execute_reply": "2023-02-01T12:09:44.585952Z",
     "shell.execute_reply.started": "2023-02-01T12:09:15.105243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd3fd2d52014fac8c6ce984311e2169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------------------+\n",
      "|movieID|count|movieTitle                   |\n",
      "+-------+-----+-----------------------------+\n",
      "|50     |583  |Star Wars (1977)             |\n",
      "|258    |509  |Contact (1997)               |\n",
      "|100    |508  |Fargo (1996)                 |\n",
      "|181    |507  |Return of the Jedi (1983)    |\n",
      "|294    |485  |Liar Liar (1997)             |\n",
      "|286    |481  |English Patient, The (1996)  |\n",
      "|288    |478  |Scream (1996)                |\n",
      "|1      |452  |Toy Story (1995)             |\n",
      "|300    |431  |Air Force One (1997)         |\n",
      "|121    |429  |Independence Day (ID4) (1996)|\n",
      "+-------+-----+-----------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "import codecs \n",
    "import os \n",
    "import boto3\n",
    "\n",
    "s3client=boto3.client(\"s3\")\n",
    "s3client.download_file(\"370784835428-datalake\",\"udemy/u.item\",\"/tmp/u.item\")\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {} \n",
    "    \n",
    "    with codecs.open(\"/tmp/u.item\",\"r\",encoding='ISO-8859-1',errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "nameDict = sc.broadcast(loadMovieNames())\n",
    "\n",
    "# create schema when reading u.data\n",
    "schema = StructType([\\\n",
    "             StructField(\"userID\", IntegerType(), True),\n",
    "             StructField(\"movieID\", IntegerType(), True),\n",
    "             StructField(\"rating\", IntegerType(), True),\n",
    "             StructField(\"timestamp\", LongType(), True)\n",
    "             ])\n",
    "\n",
    "#load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\",\"\\t\").schema(schema).csv(\"s3://370784835428-datalake/udemy/u.data\")\n",
    "\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "#create user-defined function to look up movie name from our broadcasted ditionary\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "lookupNameUDF = func.udf(lookupName)\n",
    "\n",
    "# add a movie title column using new udf \n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\",lookupNameUDF(func.col(\"movieID\")))\n",
    "\n",
    "# sort the results \n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
    "\n",
    "# grab the top 10\n",
    "sortedMoviesWithNames.show(10,False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c6537-18de-4253-a79a-3af4aede83ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
