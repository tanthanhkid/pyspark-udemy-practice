{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d41df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T08:59:05.916317Z",
     "iopub.status.busy": "2023-01-31T08:59:05.916061Z",
     "iopub.status.idle": "2023-01-31T08:59:32.687250Z",
     "shell.execute_reply": "2023-01-31T08:59:32.686693Z",
     "shell.execute_reply.started": "2023-01-31T08:59:05.916288Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404de5ff41a34624b0e0a87bcda7e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1675148691692_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-29-221.ap-southeast-1.compute.internal:20888/proxy/application_1675148691692_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-19WP3HLFA8DGF\n",
       "\" application-id=\"application_1675148691692_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-29-221.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1675148691692_0004_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "from dataclasses import dataclass, field \n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6be477-a10c-443d-8a6e-397ca2dfdc26",
   "metadata": {},
   "source": [
    "# RDD: mapValue and reduceByKey function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"s3://370784835428-datalake/udemy/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = lines.map(lambda x: x.split()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ratings.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ace9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c9de6-0e78-43a2-b347-645cc529d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"s3://370784835428-datalake/udemy/SparkCourse/SparkCourse/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine) \n",
    "\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1)).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \n",
    "\n",
    "totalsByAgeCollected = totalsByAge.collect() \n",
    "\n",
    "averagesByAge = totalsByAge.mapValues(lambda x:x[0]/x[1]) \n",
    "\n",
    "results = averagesByAge.collect()\n",
    "  \n",
    "for result in results:\n",
    "   print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14ec0-3d06-448e-94df-d837ceae1c83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e769dcf-c481-4986-aaa6-da75efcb2f4f",
   "metadata": {},
   "source": [
    "# RDD: filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f38c5-fee3-4630-8eda-20dff7ebb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d32c5-4c97-42e0-88ee-923d7fc40253",
   "metadata": {},
   "source": [
    "# RDD: flatMap function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e04dd6-7e9e-4b83-96e1-272e57adc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+',re.UNICODE).split(text.lower())\n",
    "\n",
    "input = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\n",
    "words = input.flatMap(normalizeWords) # the text split into many rows\n",
    "\n",
    "wordCounts = words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[0],x[1])).map(lambda x:(x[1],x[0]) ).sortByKey()\n",
    "# for item in wordCountsSorted.collect():# comment because the list is too long\n",
    "    # print(item)\n",
    "print(\"Result is commented because it's too long\")\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "# for result in results:\n",
    "#     count = str(result[0]) \n",
    "#     word = result[1].encode(\"ascii\",\"ignore\")\n",
    "    \n",
    "#     # comment because the list is too long\n",
    "#     if(word):\n",
    "#         print(cleanWord,count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b99a-ee39-441b-aa6c-5bdc11f10fde",
   "metadata": {},
   "source": [
    "# **DataFrame: Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dcaf8-82bc-461e-b4f8-9b7a9dabe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID = int(fields[0]), name = str(fields[1].encode(\"utf-8\")),\n",
    "               age = int(fields[2]), numFriends = int(fields[3]))\n",
    "\n",
    "lines = sc.textFile(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "\n",
    "# Infer the schema, and register the DataFrame as table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been register as a table\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >=13 and age <=19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)\n",
    "    \n",
    "# We can also use functions instead of SQL queries\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d043a-5044-4ff1-8bff-43670bb03a2c",
   "metadata": {},
   "source": [
    "# **DataFrame: Infer Schema and common function to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f44e3-250c-4793-8f76-0e0fddaa4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "people = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    "            .csv(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/fakefriends-header.csv\")\n",
    "\n",
    "print(\"here is our inferred schema\")\n",
    "people.printSchema()\n",
    "\n",
    "print(\"display the name column\")\n",
    "people.select(\"name\").show()\n",
    "\n",
    "print(\"filter out anyone over 21\")\n",
    "people.filter(people.age < 21).show()\n",
    "\n",
    "print(\"group by age\")\n",
    "people.groupBy(\"age\").count().show()\n",
    "\n",
    "print(\"make everyone 10 year older\")\n",
    "people.select(people.name, people.age +10).show()\n",
    "\n",
    "print(\"sorted\")\n",
    "friendByAge = people.select(\"age\",\"friends\")\n",
    "friendByAge.groupBy(\"age\").avg(\"friends\").sort(\"age\").show()\n",
    "\n",
    "print(\"formatted more nicely\")\n",
    "friendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2)).sort(\"age\").show()\n",
    "\n",
    "print(\"with a custom column name\")\n",
    "friendByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"),2).alias(\"friends_avg\")).sort(\"age\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611ee7b-3000-4923-9246-7baaf204104e",
   "metadata": {},
   "source": [
    "# **Word count with DataFrame(split unstructure text into multi row dataframe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333cde4-2741-4297-8073-0895cc7c702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each line of my book into a dataframe\n",
    "inputDF = spark.read.text(\"s3://thanhtt-0000-datalake/udemy/SparkCourse/book.txt\")\n",
    "inputDF.show()\n",
    "\n",
    "# split using a regular expression that extract words\n",
    "words = inputDF.select(func.explode(func.split(inputDF.value,\"\\\\W+\")).alias(\"word\"))\n",
    "words.filter(words.word != \"\")\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f4395-4d49-497e-ac2b-18422e12711e",
   "metadata": {},
   "source": [
    "# **DataFrame: StructType, StructField, IntegerType, FloatType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8577ce-cb9f-439b-8cad-a29c766a89a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "#create schema when reading customer order\n",
    "customerOrderSchema = StructType([\\\n",
    "                                 StructField(\"cust_id\", IntegerType(), True),\n",
    "                                 StructField(\"item_id\", IntegerType(), True),\n",
    "                                 StructField(\"amount_spent\", FloatType(), True)\n",
    "                                 ])\n",
    "\n",
    "#load up the data into spark dataset\n",
    "customerDF = spark.read.schema(customerOrderSchema).csv(\"s3://370784835428-datalake/udemy/SparkCourse/SparkCourse/customer-orders.csv\")\n",
    "\n",
    "totalByCustomer = customerDF.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"),2).alias(\"total_spent\"))\n",
    "\n",
    "totalByCustomerSorted = totalByCustomer.sort(\"total_spent\")\n",
    "\n",
    "totalByCustomer.show(totalByCustomerSorted.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cdf11-0424-4ed1-84c4-797d7eb2b670",
   "metadata": {},
   "source": [
    "# **DataFrame: OrderBy function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2dc918b-f6cd-428d-8705-99fcc3fcf407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T09:01:00.821160Z",
     "iopub.status.busy": "2023-01-31T09:01:00.820884Z",
     "iopub.status.idle": "2023-01-31T09:01:16.210424Z",
     "shell.execute_reply": "2023-01-31T09:01:16.209801Z",
     "shell.execute_reply.started": "2023-01-31T09:01:00.821131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb32afc8b7e469ead471e7646df3603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "+-------+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "#create schema when reading customer order\n",
    "schema = StructType([\\\n",
    "             StructField(\"userID\", IntegerType(), True),\n",
    "             StructField(\"movieID\", IntegerType(), True),\n",
    "             StructField(\"rating\", IntegerType(), True),\n",
    "             StructField(\"timestamp\", LongType(), True)\n",
    "             ])\n",
    "\n",
    "#load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\",\"\\t\").schema(schema).csv(\"s3://370784835428-datalake/udemy/u.data\")\n",
    "\n",
    "# some SQL-style magic to sort all the movies by popularity in one line\n",
    "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "# grab the top 10\n",
    "topMovieIDs.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6048ea-c8c9-4814-8c78-a89ffa569224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
